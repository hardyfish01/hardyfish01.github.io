---
title: 生产者压缩算法
categories: 
- Apache KAFKA实战
---

**Kafka是如何压缩消息的呢？**

目前Kafka共有两大类消息格式，社区分别称之为V1版本和V2版本。

> V2版本是Kafka 0.11.0.0中正式引入的。

不论是哪个版本，Kafka的消息层次都分为两层：消息集合以及消息。

一个消息集合中包含若干条日志项，而日志项才是真正封装消息的地方。

Kafka底层的消息日志由一系列消息集合日志项组成。

Kafka通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。

**那么社区引入V2版本的目的是什么呢？**

V2版本主要是针对V1版本的一些弊端做了修正，比如把消息的公共部分抽取出来放到外层消息集合里面，这样就不用每条消息都保存这些信息了。

举个例子：原来在V1版本中，每条消息都需要执行CRC校验，但有些情况下消息的CRC值是会发生变化的。

* 比如在Broker端可能会对消息时间戳字段进行更新，那么重新计算之后的CRC值也会相应更新；

* 比如Broker端在执行消息格式转换时（主要是为了兼容老版本客户端程序），也会带来CRC值的变化。

鉴于这些情况，再对每条消息都执行CRC校验就有点没必要了，不仅浪费空间还耽误CPU时间，因此在V2版本中，消息的CRC校验工作就被移到了消息集合这一层。

V2版本还有一个和压缩息息相关的改进，就是保存压缩消息的方法发生了变化。

* 之前V1版本中保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；

* V2版本的做法是对整个消息集合进行压缩，显然后者应该比前者有更好的压缩效果。

**何时压缩？**

在Kafka中，压缩可能发生在两个地方：生产者端和Broker端。

生产者程序中配置`compression.type`参数即表示启用指定类型的压缩算法。

比如下面这段程序代码展示了如何构建一个开启GZIP的Producer对象：

```java
Properties props = new Properties(); 
props.put("bootstrap.servers", "localhost:9092"); 
props.put("acks", "all"); 
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); // 开启GZIP压缩 
props.put("compression.type", "gzip"); 
Producer producer = new KafkaProducer<>(props);
```

这里比较关键的代码行是`props.put(“compression.type”, “gzip”)`，它表明该Producer的压缩算法使用的是GZIP。

这样Producer启动后生产的每个消息集合都是经GZIP压缩过的，故而能很好地节省网络传输带宽以及Kafka Broker端的磁盘占用。

有两种例外情况就可能让Broker重新压缩消息：

**情况一：Broker端指定了和Producer端不同的压缩算法。**

一旦你在Broker端设置了不同的`compression.type`值，就一定要小心了，因为可能会发生预料之外的压缩/解压缩操作，通常表现为Broker端CPU使用率飙升。

**情况二：Broker端发生了消息格式转换。**

所谓的消息格式转换主要是为了兼容老版本的消费者程序。

在一个生产环境中，Kafka集群中同时保存多种版本的消息格式非常常见。

为了兼容老版本的格式，Broker端会对新版本消息执行向老版本格式的转换。

这个过程中会涉及消息的解压缩和重新压缩。

一般情况下这种消息格式转换对性能是有很大影响的，除了这里的压缩之外，它还让Kafka丧失了Zero Copy特性。

**何时解压缩？**

通常来说解压缩发生在消费者程序中，也就是说Producer发送压缩消息到Broker后，Broker照单全收并原样保存起来。

当Consumer程序请求这部分消息时，Broker依然原样发送出去，当消息到达Consumer端后，由Consumer自行解压缩还原成之前的消息。

**基本过程：Producer端压缩、Broker端保持、Consumer端解压缩。**

注意：除了在Consumer端解压缩，Broker端也会进行解压缩。

每个压缩过的消息集合在Broker端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。

这种解压缩对Broker端性能是有一定影响的，特别是对CPU的使用率而言。

**各种压缩算法对比**

在Kafka 2.1.0版本之前，Kafka支持3种压缩算法：GZIP、Snappy和LZ4。

从2.1.0开始，Kafka正式支持Zstandard算法（简写为zstd）。

它是Facebook开源的一个压缩算法，能够提供超高的压缩比。

* 在吞吐量方面：LZ4 > Snappy > zstd和GZIP；

* 在压缩比方面，zstd > LZ4 > GZIP > Snappy。

具体到物理资源，使用Snappy算法占用的网络带宽最多，zstd最少；

在CPU使用率方面，各个算法表现得差不多，只是在压缩时Snappy算法使用的CPU较多一些，而在解压缩时GZIP算法则可能使用更多的CPU。

**最佳实践**

> 何时启用压缩是比较合适的时机呢？

启用压缩的一个条件就是Producer程序运行机器上的CPU资源要很充足。

除了CPU资源充足这一条件，如果你的环境中带宽资源有限，那么建议你开启压缩。