---
title: 集群
categories: 
- 数据库
- Redis
---

Redis Cluster 是 在 3.0 版本正式推出的高可用集群方案，相比Redis Sentinel，Redis Cluster方案不需要额外部署Sentinel集群，而是通过集群内部通信实现集群监控，故障时主从切换；同时，支持内部基于哈希实现数据分片，支持动态水平扩容

集群中有多个主节点，每个主节点有多个从节点，主从节点间数据一致，最少需要3个主节点，每个主节点最少需要1个从节点

![](https://img-blog.csdnimg.cn/af684a5f34bc412fa26990808c31cc37.png)

**Redis集群的功能限制**

- `key` **批量操作** 支持有限。

类似 `mset`、`mget` 操作，目前只支持对具有相同 `slot` 值的 `key` 执行 **批量操作**。

对于 **映射为不同** `slot` 值的 `key` 由于执行 `mget`、`mget` 等操作可能存在于多个节点上，因此不被支持。

- `key` **事务操作** 支持有限。

只支持 多 `key` 在 **同一节点上** 的 **事务操作**，当多个 `key` 分布在 **不同** 的节点上时 无法 使用事务功能。

例如，有2个key，key1 和 key2。

key1 是映射到 5500 这个 slot 上，存储在 Node A。

key2 是映射到 5501 这个 slot 上，存储在 Node B。

那么就不能对 key1 和 key2 做事务操作。

- 不支持 **多数据库空间**

单机 下的 `Redis` 可以支持 `16` 个数据库（`db0 ~ db15`），集群模式下只能使用 一个 数据库空间，即 `db0`。

**CAP取舍**

Redis Cluster选择了**AP架构**，为了保证可用性，Redis并不保证强一致性，在特定条件下会出现数据不一致甚至丢失写操作

# 数据分片

Redis Cluster首先定义了编号`0 ~ 16383`的区间，称为槽，所有的键根据哈希函数映射到`0 ~ 16383`整数槽内，计算公式：`slot=CRC16（key）&16383`。

首先根据键值对的key，按照CRC16算法计算一个16 bit的值，用这个16bit值对16384取模，得到0~16383范围内的模数，每个模数代表一个相应编号的哈希槽。

> 每一个节点负责维护一部分槽以及槽所映射的键值数据

槽是 Redis 集群管理数据的基本单位，集群扩容收缩就是槽和数据在节点之间的移动

槽与节点映射关系如下：

- 每个集群节点维护着一个`16384 bit (2kB)`的位数组，每个bit对应相同编号的槽，用 `0/1`标识对于某个槽自己是否拥有

<img src="https://img-blog.csdnimg.cn/2d84b9f8bb654f3ca4e1fdb6af238aed.png" style="zoom:25%;" />

**为什么不用一致性哈希？**

当使用 **少量节点** 时，**节点变化** 将大范围影响 **哈希环** 中 **数据映射**，不适合 **少量数据节点** 的分布式方案。

**Smart客户端** 

大多数开发语言的Redis客户端都采用Smart客户端支持集群协议

Smart客户端通过在内部维护`slot→node`的映射关系，本地就可实现键到节点的查找，从而保证IO效率的最大化，而MOVED重定向负责协助Smart客户端更新`slot→node`映射。

整个流程：

<img src="https://img-blog.csdnimg.cn/dcb1096ae81f477c862c34fb17f7f2b6.png" style="zoom:25%;" />

客户端内部维护slots缓存表，并且针对每个节点维护连接池，当集群规模非常大时，客户端会维护非常多的连接并消耗更多的内存。

**重定向机制**

客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

下面的MOVED命令响应结果，这个结果中就包含了新实例的访问地址。

```plain
GET hello:key
(error) MOVED 13320 172.16.19.5:6379
```

其中，MOVED命令表示，客户端请求的键值对所在的哈希槽13320，实际是在172.16.19.5这个实例上。

通过返回的MOVED命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。

这样一来，客户端就可以直接和172.16.19.5连接，并发送操作请求了。

由于负载均衡，Slot 2中的数据已经从实例2迁移到了实例3，但是，客户端缓存仍然记录着**Slot 2在实例2**的信息，所以会给实例2发送命令。

实例2给客户端返回一条MOVED命令，把Slot 2的最新位置（也就是在实例3上），返回给客户端，客户端就会再次向实例3发送请求，同时还会更新本地缓存，把Slot 2与实例的对应关系更新过来。

> ASK命令表示两层含义：

第一，表明Slot数据还在迁移中；第二，ASK命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例3发送ASKING命令，然后再发送操作命令。

和MOVED命令不同，**ASK命令并不会更新客户端缓存的哈希槽分配信息**。

如果客户端再次请求Slot 2中的数据，它还是会给实例2发送请求。

这也就是说，ASK命令的作用只是让客户端能给新实例发送一次请求，而不像MOVED命令那样，会更改本地缓存，让后续所有命令都发往新实例。

**在集群模式下使用mget等命令优化批量调用时，键列表必须具有相同的slot，否则会报错。**

cluster keyslot命令就是采用`key_hash_slot`函数实现的

```
127.0.0.1:6379> cluster keyslot key:test:111
(integer) 10050 
127.0.0.1:6379> cluster keyslot key:{hash_tag}:111
(integer) 2515 
127.0.0.1:6379> cluster keyslot key:{hash_tag}:222
(integer) 2515
```

其中键内部使用大括号包含的内容又叫做`hash_tag`，它提供不同的键可以具备相同slot的功能，常用于Redis IO优化。

这时可以利用`hash_tag`让不同的键具有相同的slot达到优化的目的。

```
127.0.0.1:6385> mget user:10086:frends user:10086:videos 
(error) CROSSSLOT Keys in request don't hash to the same slot 
127.0.0.1:6385> mget user:{10086}:friends user:{10086}:videos 
1) "friends"
2) "videos"
```

Pipeline同样可以受益于hash_tag，由于Pipeline只能向一个节点批量发送执行命令，而相同slot必然会对应到唯一的节点，降低了集群使用Pipeline的门槛。

# 主从复制

参与复制的Redis实例划分为主节点(master)和从节点(slave)。默认情况下，Redis都是主节点。

每个从节点只能有一个主节点，而主节点可以 同时具有多个从节点。

复制的数据流是单向的，只能由主节点复制到从节点。

默认情况下，从节点使用slave-read-only=yes配置为只读模式。

由于复制只能从主节点到从节点，对于从节点的任何修改主节点都无法感知，修改从节点会造成主从数据不一致。

**因此建议线上不要修改从节点的只读模式。**

**传输延迟**

主从节点一般部署在不同机器上，复制时的网络延迟就成为需要考虑的问题，Redis为我们提供了repl-disable-tcp-nodelay参数用于控制是否关闭 `TCP_NODELAY`，默认关闭，说明如下:

* 当关闭时，主节点产生的命令数据无论大小都会及时地发送给从节点，这样主从之间延迟会变小，但增加了网络带宽的消耗，适用于主从之间 的网络环境良好的场景，如同机架或同机房部署。

* 当开启时，主节点会合并较小的TCP数据包从而节省带宽。默认发送 时间间隔取决于Linux的内核，一般默认为40毫秒，这种配置节省了带宽但 增大主从之间的延迟。适用于主从网络环境复杂或带宽紧张的场景，如跨机房部署。

部署主从节点时需要考虑网络延迟、带宽使用率、防灾级别等因素，如要求低延迟时，建议同机架或同机房部署并关闭repl-disable-tcp-nodelay;

如果考虑高容灾性，可以同城跨机房部署并开启repl-disable-tcp-nodelay。

**断开复制**

slaveof命令不但可以建立复制，还可以在从节点执行slaveof no one来断 开与主节点复制关系。

在从节点执行slaveof命令后，复制过程便开始运作。

**复制过程大致分为6个过程:**

> 保存主节点(master)信息。

执行slaveof后从节点只保存主节点的地址信息便直接返回，这时建立复制流程还没有开始。

<img src="https://img-blog.csdnimg.cn/a474136a04ce43ea8279ca4091512e7d.png" style="zoom:25%;" />

从节点(slave)内部通过每秒运行的定时任务维护复制相关逻辑， 当定时任务发现存在新的主节点后，会尝试与该节点建立网络连接

从节点会建立一个socket套接字，专门用于接受主节点发送的复制命令。

如果从节点无法建立连接，定时任务会无限重试直到连接成功或者执行 slaveof no one取消复制。

> 连接失败，可以在从节点执行info replication查看` master_link_down_since_seconds`指标，它会记录与主节点连接失败的系统时 间。

发送ping命令，连接建立成功后从节点发送ping请求进行首次通信，ping请求主要目的如下: 

* 检测主从之间网络套接字是否可用。

* 检测主节点当前是否可接受处理命令。

如果发送ping命令后，从节点没有收到主节点的pong回复或者超时，比 如网络超时或者主节点正在阻塞无法响应命令，从节点会断开复制连接，下次定时任务会发起重连。

权限验证。如果主节点设置了requirepass参数，则需要密码验证， 从节点必须配置masterauth参数保证与主节点相同的密码才能通过验证，如 果验证失败复制将终止，从节点重新发起复制流程。

同步数据集。主从复制连接正常通信后，对于首次建立复制的场 景，主节点会把持有的数据全部发送给从节点，这部分操作是耗时最长的步 骤。

> Redis在2.8版本以后采用新复制命令psync进行数据同步，原来的sync命令依然支持，保证新旧版本的兼容性。

命令持续复制。当主节点把当前的数据同步给从节点后，便完成了复制的建立流程。

接下来主节点会持续地把写命令发送给从节点，保证主从数据一致性。

Redis在2.8及以上版本使用psync命令完成主从数据同步，同步过程分为:全量复制和部分复制。

**全量复制:**

一般用于初次复制场景，Redis早期支持的复制功能只有全量复制，它会把主节点全部数据一次性发送给从节点，当数据量较大时，会对主从节点和网络造成很大的开销。

**部分复制:**

用于处理在主从复制中因网络闪断等原因造成的数据丢失场景，当从节点再次连上主节点后，如果条件允许，主节点会补发丢失数据给从节点。因为补发的数据远远小于全量数据，可以有效避免全量复制的过高开销。

部分复制是对老版复制的重大优化，有效避免了不必要的全量复制操作。

> 因此当使用复制功能时，尽量采用2.8以上版本的Redis。

**复制偏移量** 

参与复制的主从节点都会维护自身复制偏移量。

主节点(master)在处理完写入命令后，会把命令的字节长度做累加记录，统计信息在info relication中的`master_repl_offset`指标中

从节点(slave)每秒钟上报自身的复制偏移量给主节点，因此主节点 也会保存从节点的复制偏移量

从节点在接收到主节点发送的命令后，也会累加记录自身的偏移量。

统计信息在info relication中的`slave_repl_offset`指标中

通过对比主从节点的复制偏移量，可以判断主从节点数据是否一致。

可以通过主节点的统计信息，计算出`master_repl_offset-slave_offset`字节 量，判断主从节点复制相差的数据量，根据这个差值判定当前复制的健康 度。

如果主从之间复制偏移量相差较大，则可能是网络延迟或命令阻塞等原因引起。

**复制积压缓冲区**

复制积压缓冲区是保存在主节点上的一个固定长度的队列，默认大小为 1MB，当主节点有连接的从节点(slave)时被创建，这时主节点(master) 响应写命令时，不但会把命令发送给从节点，还会写入复制积压缓冲区

由于缓冲区本质上是先进先出的定长队列，所以能实现保存最近已复制数据的功能，用于部分复制和复制命令丢失的数据补救。

复制缓冲区相关统计信息保存在主节点的info replication中

**主节点运行ID**

每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运 行ID。

运行ID的主要作用是用来唯一识别Redis节点，比如从节点保存主节点的运行ID识别自己正在复制的是哪个主节点。

如果只使用ip+port的方式识 别主节点，那么主节点重启变更了整体数据集(如替换RDB/AOF文件)， 从节点再基于偏移量复制数据将是不安全的，因此当运行ID变化后从节点将 做全量复制。可以运行info server命令查看当前节点的运行ID

> 需要注意的是Redis关闭再启动后，运行ID会随之改变

**psync命令** 

从节点使用psync命令完成部分复制和全量复制功能，命令格式：`psync{runId}{offset}`，参数含义如下:

* runId:从节点所复制主节点的运行id。

* offset:当前从节点已复制的数据偏移量。

<img src="https://img-blog.csdnimg.cn/9066ce62621a4aa79b2a391ddf06a26a.png" style="zoom:25%;" />

从节点(slave)发送psync命令给主节点，参数runId是当前从节点保存的主节点运行ID，参数offset是当前从节点保存的复制偏移量，如果是第一次参与复制则默认值为-1。

主节点(master)根据psync参数和自身数据情况决定响应结果: 

* 如果回复+FULLRESYNC{runId}{offset}，那么从节点将触发全量复制流程。

* 如果回复+CONTINUE，从节点将触发部分复制流程。

* 如果回复+ERR，说明主节点版本低于Redis2.8，无法识别psync命令， 从节点将发送旧版的sync命令触发全量复制流程。

## 全量复制

全量复制是Redis最早支持的复制方式，也是主从第一次建立复制时必须经历的阶段。触发全量复制的命令是sync和psync

<img src="https://img-blog.csdnimg.cn/3d874a866edd4885b85437704fb4d6e2.png" style="zoom:25%;" />

**全量复制的完整运行流程说明:**

1.发送psync命令进行数据同步，由于是第一次进行复制，从节点没有 复制偏移量和主节点的运行ID，所以发送psync=-1。

2.主节点根据`psync=-1`解析出当前为全量复制，回复+FULLRESYNC响 应。

3.从节点接收主节点的响应数据保存运行ID和偏移量offset

<img src="https://img-blog.csdnimg.cn/98098713fffd4e60b2e6137ef10480d6.png" style="zoom:25%;" />

4.主节点执行bgsave保存RDB文件到本地。

5.主节点发送RDB文件给从节点，从节点把接收的RDB文件保存在本地并直接作为从节点的数据文件

> 需要注意，对于数据量较大的主节点，比如生成的RDB文件超过6GB以 上时要格外小心。

传输文件这一步操作非常耗时，速度取决于主从节点之间网络带宽。

如果总时间超过 repl-timeout所配置的值(默认60秒)，从节点将放弃接受RDB文件并清理已经下载的临时文件，导致全量复制失败

针对数据量较大的节点，建议调大repl-timeout参数防止出现全量同步数据超时。

**关于无盘复制:**

为了降低主节点磁盘开销，Redis支持无盘复制，生成 的RDB文件不保存到硬盘而是直接通过网络发送给从节点，通过repl- diskless-sync参数控制，默认关闭。

> 对于从节点开始接收RDB快照到接收完成期间，主节点仍然响应读 写命令，因此主节点会把这期间写命令数据保存在复制客户端缓冲区内，当从节点加载完RDB文件后，主节点再把缓冲区内的数据发送给从节点，保证主从之间数据一致性。

如果主节点创建和传输RDB的时间过长，对于高流量写入场景非常容易造成主节点复制客户端缓冲区溢出。

默认配置为`client- output-buffer-limit slave 256MB 64MB 60`，如果60秒内缓冲区消耗持续大于 64MB或者直接超过256MB时，主节点将直接关闭复制客户端连接，造成全量同步失败。

因此，运维人员需要根据主节点数据量和写命令并发量调整`client- output-buffer-limit slave`配置，避免全量复制期间客户端缓冲区溢出。

> 从节点接收完主节点传送来的全部数据后会清空自身旧数据

从节点清空数据后开始加载RDB文件，对于较大的RDB文件，这一 步操作依然比较耗时，可以通过计算日志之间的时间差来判断加载RDB的总耗时

对于线上做读写分离的场景，从节点也负责响应读命令。

如果此时从节 点正出于全量复制阶段或者复制中断，那么从节点在响应读命令可能拿到过 期或错误的数据。

对于这种场景，Redis复制提供了slave-serve-stale-data参 数，默认开启状态。如果开启则从节点依然响应所有命令。

对于无法容忍不 一致的应用场景可以设置no来关闭命令执行，此时从节点除了info和slaveof 命令之外所有的命令只返回**SYNC with master in progress**信息。

> 从节点成功加载完RDB后，如果当前节点开启了AOF持久化功能， 它会立刻做bgrewriteaof操作，为了保证全量复制后AOF持久化文件立刻可用。

<img src="https://img-blog.csdnimg.cn/15abe69e41c446d2bd72bec9638dd284.png" style="zoom:15%;" />

**规避全量复制**

> 节点运行ID不匹配:

当主从复制关系建立后，从节点会保存主节点的 运行ID，如果此时主节点因故障重启，那么它的运行ID会改变，从节点发现 主节点运行ID不匹配时，会认为自己复制的是一个新的主节点从而进行全量 复制。

对于这种情况应该从架构上规避，比如提供故障转移功能。当主节点 发生故障后，手动提升从节点为主节点或者采用支持自动故障转移的哨兵或 集群方案。

> 复制积压缓冲区不足:

当主从节点网络中断后，从节点再次连上主节 点时会发送psync{offset}{runId}命令请求部分复制，如果请求的偏移量不在 主节点的积压缓冲区内，则无法提供给从节点数据，因此部分复制会退化为 全量复制。

针对这种情况需要根据网络中断时长，写命令数据量分析出合理 的积压缓冲区大小。

网络中断一般有闪断、机房割接、网络分区等情况。这 时网络中断的时长一般在分钟级(net_break_time)。

积压缓冲区默认为1MB，对于大流量场景显然 不够，这时需要增大积压缓冲区，保证 `repl_backlog_size>net_break_time*write_size_per_minute`，从而避免因复制积压缓冲区不足造成的全量复制。

## 部分复制

在Redis 2.8之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。

从Redis 2.8开始，网络断了之后，主从库会采用增量复制的方式继续同步。

当从节点(slave)正在复制主节点 (master)时，如果出现网络闪断或者命令丢失等异常情况时，从节点会向主节点要求补发丢失的命令数据，如果主节点的**复制积压缓冲区**内存在这部分数据则直接发送给从节点，这样就可以保持主从节点复制的一致性。

补发 的这部分数据一般远远小于全量数据，所以开销很小。

**部分复制的流程**

1. 当主从节点之间网络出现中断时，如果超过repl-timeout时间，主节点会认为从节点故障并中断复制连接

2. 主从连接中断期间主节点依然响应命令，但因复制连接中断命令无法发送给从节点，不过主节点内部存在的复制积压缓冲区，依然可以保存最 近一段时间的写命令数据，默认最大缓存1MB。

3. 当主从节点网络恢复后，从节点会再次连上主节点

4. 当主从连接恢复后，由于从节点之前保存了自身已复制的偏移量和主节点的运行ID。因此会把它们当作psync参数发送给主节点，要求进行部 分复制操作。

5. 主节点接到psync命令后首先核对参数runId是否与自身一致，如果一 致，说明之前复制的是当前主节点;

> 之后根据参数offset在自身复制积压缓冲区查找，如果偏移量之后的数据存在缓冲区中，则对从节点发送+CONTINUE响应，表示可以进行部分复制。

6. 主节点根据偏移量把复制积压缓冲区里的数据发送给从节点，保证主从复制进入正常状态。

**`repl_backlog_buffer复制积压`缓冲区**

当主从库断连后，主库会把断连期间收到的写操作命令，写入replication buffer，同时也会把这些操作命令也写入`repl_backlog_buffer`这个缓冲区。

`repl_backlog_buffer`是一个环形缓冲区，**主库会记录自己写到的位置，从库则会记录自己已经读到的位置**。

刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。

随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是`master_repl_offset`。主库接收的新写操作越多，这个值就会越大。

同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量`slave_repl_offset`也在不断增加。正常情况下，这两个偏移量基本相等。

主从库的连接恢复之后，从库首先会给主库发送psync命令，并把自己当前的`slave_repl_offset`发给主库，主库会判断自己的master_repl_offset和slave_repl_offset之间的差距。

在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，`master_repl_offset`会大于slave_repl_offset。此时，主库只用把master_repl_offset和slave_repl_offset之间的命令操作同步给从库就行。

因为repl_backlog_buffer是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。

**如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致**。

<img src="https://img-blog.csdnimg.cn/9656b19731a34e2b845e33bec85ecdcd.png" style="zoom:15%;" />

## 心跳

主从节点在建立复制后，它们之间维护着长连接并彼此发送心跳命令

**主从心跳判断机制:**

1. 主从节点彼此都有心跳检测机制，各自模拟成对方的客户端进行通信。

2. 主节点默认每隔10秒对从节点发送ping命令，判断从节点的存活性 和连接状态。可通过参数repl-ping-slave-period控制发送频率。

3. 从节点在主线程中每隔1秒发送replconf ack{offset}命令，给主节点 上报自身当前的复制偏移量。

replconf命令主要作用如下:

* 实时监测主从节点网络状态。 
* 上报自身复制偏移量，检查复制数据是否丢失，如果从节点数据丢失，再从主节点的复制缓冲区中拉取丢失数据。 
* 实现保证从节点的数量和延迟性功能，通过min-slaves-to-write、min-slaves-max-lag参数配置定义。

主节点根据replconf命令判断从节点超时时间，体现在info replication统 计中的lag信息中，lag表示与从节点最后一次通信延迟的秒数，正常延迟应 该在0和1之间。

如果超过repl-timeout配置的值(默认60秒)，则判定从节点 下线并断开复制客户端连接。即使主节点判定从节点下线后，如果从节点重 新恢复，心跳检测会继续进行。

为了降低主从延迟，一般把Redis主从节点部署在相同的机房/同城机 房，避免网络延迟和网络分区造成的心跳中断等情况。

## 异步复制

主节点不但负责数据读写，还负责把写命令同步给从节点。

写命令的发 送过程是异步完成，也就是说主节点自身处理完写命令后直接返回给客户端，并不等待从节点复制完成

**复制流程:**

1. 主节点6379接收处理命令。

2. 命令处理完之后返回响应结果。

3. 对于修改命令异步发送给6380从节点，从节点在主线程中执行复制 的命令。

由于主从复制过程是异步的，就会造成从节点的数据相对主节点存在延 迟。

具体延迟多少字节，我们可以在主节点执行info replication命令查看相关指标获得。

## 复制风暴

复制风暴是指大量从节点对同一主节点或者对同一台机器的多个主节点 短时间内发起全量复制的过程。

复制风暴对发起复制的主节点或者机器造成 大量开销，导致CPU、内存、带宽消耗。

因此我们应该分析出复制风暴发生 的场景，提前采用合理的方式规避。规避方式有如下几个。

> 1.单主节点复制风暴

单主节点复制风暴一般发生在主节点挂载多个从节点的场景。当主节点 重启恢复后，从节点会发起全量复制流程，这时主节点就会为从节点创建 RDB快照，如果在快照创建完毕之前，有多个从节点都尝试与主节点进行全量同步，那么其他从节点将共享这份RDB快照。

这点Redis做了优化，有效 避免了创建多个快照。但是，同时向多个从节点发送RDB快照，可能使主节点的网络带宽消耗严重，造成主节点的延迟变大，极端情况会发生主从节点连接断开，导致复制失败。

解决方案首先可以减少主节点(master)挂载从节点(slave)的数量， 或者采用树状复制结构，加入中间层从节点用来保护主节点

<img src="https://img-blog.csdnimg.cn/de04377213e743bd9bb138f00e66b1b5.png" style="zoom:15%;" />

从节点采用树状树非常有用，网络开销交给位于中间层的从节点，而不必消耗顶层的主节点。但是这种树状结构也带来了运维的复杂性，增加了手 动和自动处理故障转移的难度。

> 2.单机器复制风暴 

由于Redis的单线程架构，通常单台机器会部署多个Redis实例。

当一台机器(machine)上同时部署多个主节点(master)时

如果这台机器出现故障或网络长时间中断，当它重启恢复后，会有大量从节点(slave)针对这台机器的主节点进行全量复制，会造成当前机器网 络带宽耗尽。

**如何避免?**

* 应该把主节点尽量分散在多台机器上，避免在单台机器上部署过多的主节点。

* 当主节点所在机器故障后提供故障转移机制，避免机器恢复后进行密集的全量复制。

# 主从一致性

**读取过期数据**

如果客户端从主库上读取留存的过期数据，主库会触发删除操作，此时，客户端并不会读到过期数据。

但是，从库本身不会执行删除操作，如果客户端在从库中访问留存的过期数据，从库并不会触发数据删除。那么，从库会给客户端返回过期数据吗？

如果你使用的是Redis 3.2之前的版本，那么，从库在服务读请求时，并不会判断数据是否过期，而是会返回过期数据。

在3.2版本后，Redis做了改进，如果读取的数据已经过期了，从库虽然不会删除，但是会返回空值，这就避免了客户端读到过期数据。

所以，**在应用主从集群时，尽量使用Redis 3.2及以上版本**。

只要使用了Redis 3.2后的版本，就不会读到过期数据了吗？其实还是会的。

> 这跟Redis用于设置过期时间的命令有关系，有些命令给数据设置的过期时间在从库上可能会被延后，导致应该过期的数据又在从库上被读取到了。

为了避免这种情况，我给你的建议是，**在业务应用中使用EXPIREAT/PEXPIREAT命令，把数据的过期时间设置为具体的时间点，避免读到过期数据。**

**不合理配置项导致的服务挂掉**

**1.protected-mode配置项**

这个配置项的作用是限定哨兵实例能否被其他服务器访问。

当这个配置项设置为yes时，哨兵实例只能在部署的服务器本地进行访问。当设置为no时，其他服务器也可以访问这个哨兵实例。

正因为这样，如果protected-mode被设置为yes，而其余哨兵实例部署在其它服务器，那么，这些哨兵实例间就无法通信。

当主库故障时，哨兵无法判断主库下线，也无法进行主从切换，最终Redis服务不可用。

**2.cluster-node-timeout配置项**

**这个配置项设置了Redis Cluster中实例响应心跳消息的超时时间**。

当我们在Redis Cluster集群中为每个实例配置了**一主一从**模式时，如果主实例发生故障，从实例会切换为主实例，受网络延迟和切换操作执行的影响，切换时间可能较长，就会导致实例的心跳超时（超出cluster-node-timeout）。

实例超时后，就会被Redis Cluster判断为异常。而Redis Cluster正常运行的条件就是，有半数以上的实例都能正常运行。

所以，如果执行主从切换的实例超过半数，而主从切换时间又过长的话，就可能有半数以上的实例心跳超时，从而可能导致整个集群挂掉。所以，**我建议你将cluster-node-timeout调大些（例如10到20秒）**。

# 脑裂问题

所谓的脑裂，就是指在主从集群中，同时有两个主节点，它们都能接收写请求。

脑裂最直接的影响，就是客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据。

严重的话，脑裂会进一步导致数据丢失。

**为什么会发生脑裂？**

原主库假故障导致的脑裂

脑裂发生的原因主要是原主库发生了假故障，我们来总结下假故障的两个原因：

1. 和主库部署在同一台服务器上的其他程序临时占用了大量资源（例如CPU资源），导致主库资源使用受限，短时间内无法响应心跳。其它程序不再使用资源时，主库又恢复正常。
2. 主库自身遇到了阻塞的情况，例如，处理bigkey或是发生内存swap，短时间内无法响应心跳，等主库阻塞解除后，又恢复正常的请求处理了。

**为什么脑裂会导致数据丢失？**

在主从切换的过程中，如果原主库只是**假故障**，它会触发哨兵启动主从切换，一旦等它从假故障中恢复后，又开始处理请求，这样一来，就会和新主库同时存在，形成脑裂。

主从切换后，从库一旦升级为新主库，哨兵就会让原主库执行slave of命令，和新主库重新进行全量同步。而在全量同步执行的最后阶段，原主库需要清空本地的数据，加载新主库发送的RDB文件，这样一来，原主库在主从切换期间保存的新写数据就丢失了。

**如何应对脑裂问题？**

Redis已经提供了两个配置项来限制主库的请求处理，分别是min-slaves-to-write和min-slaves-max-lag。

- min-slaves-to-write：这个配置项设置了主库能进行数据同步的最少从库数量；

- min-slaves-max-lag：这个配置项设置了主从库间进行数据复制时，从库给主库发送ACK消息的最大延迟（以秒为单位）。

我们可以把min-slaves-to-write和min-slaves-max-lag这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为N和T。

这两个配置项组合后的要求是，主库连接的从库中至少有N个从库，和主库进行数据复制时的ACK消息延迟不能超过T秒，否则，主库就不会再接收客户端的请求了。

即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行ACK确认了。

这样一来，min-slaves-to-write和min-slaves-max-lag的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不能在原主库中写入新数据了。

等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。

# 通信

在分布式存储中需要提供维护节点元数据信息的机制，所谓元数据是指:节点负责哪些数据，是否出现故障等状态信息。

常见的元数据维护方式分为:集中式和P2P方式。

Redis集群采用P2P的Gossip(流言)协议， Gossip协议工作原理就是节点彼此不断通信交换信息，一段时间后所有的节点都会知道集群完整的信息，这种方式类似流言传播

**实例间的通信开销会随着实例规模增加而增大**，在集群超过一定规模时（比如800节点），集群吞吐量反而会下降。

所以，集群的实际规模会受到限制。

Redis Cluster在运行时，每个实例上都会保存Slot和实例的对应关系（也就是Slot映射表），以及自身的状态信息。

为了让集群中的每个实例都知道其它所有实例的状态信息，实例之间会按照一定的规则进行通信。这个规则就是Gossip协议。

> 集群中的每个节点都会单独开辟一个TCP通道，用于节点之间彼此通信，通信端口号在基础端口上加10000。

**Gossip协议的工作原理可以概括成两点：**

-  一是，每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把PING消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及Slot映射表。 

-  二是，一个实例在接收到PING消息后，会给发送PING消息的实例，发送一个PONG消息。PONG消息包含的内容和PING消息一样。 

下图显示了两个实例间进行PING、PONG消息传递的情况：

<img src="https://img-blog.csdnimg.cn/e74311d4d53243cab827d594d3e53593.png" style="zoom:25%;" />

Gossip协议可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息。

这样一来，即使有新节点加入、节点故障、Slot变更等事件发生，实例间也可以通过PING、PONG消息的传递，完成集群状态在每个实例上的同步。

经过刚刚的分析，我们可以很直观地看到，实例间使用Gossip协议进行通信时，通信开销受到**通信消息大小**和**通信频率**这两方面的影响，

消息越大、频率越高，相应的通信开销也就越大。如果想要实现高效的通信，可以从这两方面入手去调优。

**Gossip消息** 

常用的Gossip消息可分为:ping消息、pong消息、meet消息、fail消息等。

* meet消息：用于通知新节点加入。

* ping消息：集群内交换最频繁的消息，集群内每个节点每秒向多个其他节点发送ping消息，用于检测节点是否在线和交换彼此状态信息。ping消 息发送封装了自身节点和部分其他节点的状态数据。

* pong消息：当接收到ping、meet消息时，作为响应消息回复给发送方确认消息正常通信。

* fail消息：当节点判定集群内另一个节点下线时，会向集群内广播一个 fail消息，其他节点接收到fail消息之后把对应节点更新为下线状态。

# 故障转移

**主观下线 (pfail)和客观下线(fail)**

* 主观下线：指某个节点认为另一个节点不可用，即下线状态，这个状 态并不是最终的故障判定，只能代表一个节点的意见，可能存在误判情况。

* 客观下线：指标记一个节点真正的下线，集群内多个节点都认为该节 点不可用，从而达成共识的结果。

> 1.主观下线

集群中每个节点都会定期向其他节点发送ping消息，接收节点回复pong 消息作为响应。

如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接收节点标记为主观下线(pfail)状态。

<img src="https://img-blog.csdnimg.cn/8f0a71488fb64fd59c1a079f66280038.png" style="zoom:25%;" />

**流程说明:** 

1. 节点a发送ping消息给节点b，如果通信正常将接收到pong消息，节点a更新最近一次与节点b的通信时间。 
2. 如果节点a与节点b通信出现问题则断开连接，下次会进行重连。如果一直通信失败，则节点a记录的与节点b最后通信时间将无法更新。 
3. 节点a内的定时任务检测到与节点b最后通信时间超高cluster-node-timeout时，更新本地对节点b的状态为主观下线(pfail)。

> 2.客观下线

当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播。

ping/pong消息的消息体会携带集群1/10的其他节点状态数据， 当接受节点发现消息体中含有主观下线的节点状态时，会在本地找到故障节点的ClusterNode结构，保存到下线报告链表中。

当半数以上持有槽的主节点都标记某个节点是主观下线时。触发客观下线流程。

**这里有两个问题:**

1. 为什么必须是负责槽的主节点参与故障发现决策?

   因为集群模式下 只有处理槽的主节点才负责读写请求和集群槽等关键信息维护，而从节点只进行主节点数据和状态信息的复制。

2. 为什么半数以上处理槽的主节点?

   必须半数以上是为了应对网络分区等原因造成的集群分割情况，被分割的小集群因为无法完成从主观下线到客观下线这一关键过程，从而防止小集群完成故障转移之后继续对外提供服务。

下线报告的有效期限是`server.cluster_node_timeout*2`，主要是针对故障误报的情况。

<img src="https://img-blog.csdnimg.cn/004e6299e2984036bb06c4280f6ab7cd.png" style="zoom:25%;" />

# 故障恢复

故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它的从节点中选出一个替换它，从而保证集群的高可用。

> 1.资格检查 

每个从节点都要检查最后与主节点断线时间，判断是否有资格替换故障的主节点。

如果从节点与主节点断线时间超过`cluster-node-time*cluster-slave-validity-factor`，则当前从节点不具备故障转移资格。

参数cluster-slave- validity-factor用于从节点的有效因子，默认为10。

> 2.准备选举时间 

当从节点符合故障转移资格后，更新触发故障选举的时间，只有到达该时间后才能执行后续流程。

采用延迟触发机制，主要是通过对多个从节点使用不同的延迟选举时间来支持优先级问题。

复制偏移量越大说明从节点延迟越低，那么 它应该具有更高的优先级来替换故障主节点。

主节点b进入客观下线后，它的三个从节点根据自身复制偏移量设置延迟选举时间，如复制偏移量最大的节点slave b-1延迟1秒执行，保证复制延 迟低的从节点优先发起选举。

<img src="https://img-blog.csdnimg.cn/5f8d923333b943d9804a04307596a7de.png" style="zoom:25%;" />

> 3.选举投票

投票过程其实是一个领导者选举的过程，如集群内有N个持有槽的主节点代表有N张选票。

由于在每个配置纪元内持有槽的主节点只能投票给一个 从节点，因此只能有一个从节点获得N/2+1的选票，保证能够找出唯一的从节点。

Redis集群没有直接使用从节点进行领导者选举，主要因为从节点数必须大于等于3个才能保证凑够N/2+1个节点，将导致从节点资源浪费。使用 集群内所有持有槽的主节点进行领导者选举，即使只有一个从节点也可以完成选举过程。

当从节点收集到N/2+1个持有槽的主节点投票时，从节点可以执行替换主节点操作，例如集群内有5个持有槽的主节点，主节点b故障后还有4个， 当其中一个从节点收集到3张投票时代表获得了足够的选票可以进行替换主节点操作。

<img src="https://img-blog.csdnimg.cn/ef1b8998afda455c9764588a16f344ae.png" style="zoom:25%;" />

故障主节点也算在投票数内，假设集群内节点规模是3主3从，其中有2 个主节点部署在一台机器上，当这台机器宕机时，由于从节点无法收集到 3/2+1个主节点选票将导致故障转移失败。

这个问题也适用于故障发现环节。因此部署集群时所有主节点最少需要部署在3台物理机上才能避免单点 问题。

**投票作废:**

每个配置纪元代表了一次选举周期，如果在开始投票之后的 `cluster-node-timeout*2`时间内从节点没有获取足够数量的投票，则本次选举作废。从节点对配置纪元自增并发起下一轮投票，直到选举成功为止。

> 4.替换主节点

  当从节点收集到足够的选票之后，触发替换主节点操作:

1. 当前从节点取消复制变为主节点。

2. 执行clusterDelSlot操作撤销故障主节点负责的槽，并执行 clusterAddSlot把这些槽委派给自己。

3. 向集群广播自己的pong消息，通知集群内所有的节点当前从节点变为主节点并接管了故障主节点的槽信息。